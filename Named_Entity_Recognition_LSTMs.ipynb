{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6czvz5VKO5M"
      },
      "source": [
        "# Notebook for Programming in Problem 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o8HI5JqTvU5"
      },
      "source": [
        "## Learning Objectives\n",
        "In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the same dataset and boilerplate code as in Programming Problem 1 of Assignment #3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrHyvWvTyGZ"
      },
      "source": [
        "## Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r6YTnpgbFdMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e60dec22-0090-45c7-b34c-3af064ea4fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr 25 23:22:49 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # you may need to try reconnecting to get a T4 gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYMKJlKNXYe"
      },
      "source": [
        "## Installing PyTorch and Other Packages\n",
        "\n",
        "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-dRVuiP_JVdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5c336a-fea6-415d-be71-6b79c4d47039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPsFH637OpLy"
      },
      "source": [
        "Test if our installation works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c62StNb2NvKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95879657-d25b-4a09-9538-8cf92378a9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch successfully installed!\n",
            "Version: 2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Multiply two matrices on GPU\n",
        "a = torch.rand(100, 200).cuda()\n",
        "b = torch.rand(200, 100).cuda()\n",
        "c = torch.matmul(a, b)\n",
        "\n",
        "print(\"PyTorch successfully installed!\")\n",
        "print(\"Version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qaC8sxcqkGX"
      },
      "source": [
        "Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i5Y2xB_uqqM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc682e5-50e9-4db3-c48b-a0899c192b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhV4CYivRbt4"
      },
      "source": [
        "Let's import all the packages at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EjRM4cCFRh-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import Vocab, vocab\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1bIPjAN-9V"
      },
      "source": [
        "## Long Short Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOKIneRTrTH"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lWqz7kDxSqeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6eb2424-b123-4fab-976a-ad06a00609bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP I-NP PER\n",
            "Blackburn NNP I-NP PER\n",
            "\n",
            "BRUSSELS NNP I-NP LOC\n",
            "1996-08-22 CD I-NP O\n",
            "\n",
            "The DT I-NP O\n",
            "European NNP I-NP ORG\n",
            "Commission NNP I-NP ORG\n",
            "said VBD I-VP O\n",
            "on IN I-PP O\n",
            "Thursday NNP I-NP O\n",
            "it PRP B-NP O\n",
            "disagreed VBD I-VP O\n",
            "with IN I-PP O\n",
            "German JJ I-NP MISC\n",
            "advice NN I-NP O\n",
            "to TO I-PP O\n",
            "consumers NNS I-NP O\n",
            "to TO I-VP O\n",
            "shun VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            "until IN I-SBAR O\n",
            "scientists NNS I-NP O\n",
            "determine VBP I-VP O\n",
            "whether IN I-SBAR O\n",
            "mad JJ I-NP O\n",
            "cow NN I-NP O\n",
            "disease NN I-NP O\n",
            "can MD I-VP O\n",
            "be VB I-VP O\n",
            "transmitted VBN I-VP O\n",
            "to TO I-PP O\n",
            "sheep NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Germany NNP I-NP LOC\n",
            "'s POS B-NP O\n",
            "representative NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "!cat eng.train | head -n 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVt1a6nzWsiF"
      },
      "source": [
        "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WnNfOBUYJvVW"
      },
      "outputs": [],
      "source": [
        "# A sentence is a list of (word, tag) tuples.\n",
        "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
        "Sentence = List[Tuple[str, str]]\n",
        "\n",
        "\n",
        "def read_data_file(\n",
        "    datapath: str,\n",
        ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Read and preprocess input data from the file `datapath`.\n",
        "    Example:\n",
        "    ```\n",
        "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
        "    ```\n",
        "    Return values:\n",
        "        `sentences`: a list of sentences, including words and NER tags\n",
        "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
        "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
        "    \"\"\"\n",
        "    sentences: List[Sentence] = []\n",
        "    word_cnt: Dict[str, int] = Counter()\n",
        "    tag_cnt: Dict[str, int] = Counter()\n",
        "\n",
        "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
        "        if \"DOCSTART\" in sentence_txt:\n",
        "            # Ignore dummy sentences at the begining of each document.\n",
        "            continue\n",
        "        # Read a new sentence\n",
        "        sentences.append([])\n",
        "        for token in sentence_txt.split(\"\\n\"):\n",
        "            w, _, _, t = token.split()\n",
        "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
        "            w = re.sub(\"\\d\", \"0\", w)\n",
        "            word_cnt[w] += 1\n",
        "            tag_cnt[t] += 1\n",
        "            sentences[-1].append((w, t))\n",
        "\n",
        "    return sentences, word_cnt, tag_cnt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WLMGYSZ7KxzP"
      },
      "outputs": [],
      "source": [
        "# Some helper code\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Use GPU when it is available; use CPU otherwise.\n",
        "    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n",
        "    \"\"\"\n",
        "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wVHAOb7iMPwC"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate various evaluation metrics such as accuracy and F1 score\n",
        "    Parameters:\n",
        "        `ground_truth`: the list of ground truth NER tags\n",
        "        `predictions`: the list of predicted NER tags\n",
        "    \"\"\"\n",
        "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
        "        \"f1\": f1_scores,\n",
        "        \"average f1\": np.mean(f1_scores),\n",
        "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s830dhbnj1L"
      },
      "source": [
        "## Long Short-term Memory (LSTM)\n",
        "\n",
        "Now we implement an one-layer LSTM for the same task and compare it to FFNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to7DnWNiY5ZS"
      },
      "source": [
        "### Data Loading **(4 points)**\n",
        "\n",
        "Like before, we first implement the data loader. But unlike before, each data example is now a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J5oVgqE7JaJp"
      },
      "outputs": [],
      "source": [
        "# 3 sentences with different lengths\n",
        "sentence_1 = torch.tensor([6, 1, 2])\n",
        "sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n",
        "sentence_3 = torch.tensor([3, 4])\n",
        "# Form a batch by padding 0\n",
        "sentence_batch = torch.tensor([\n",
        "    [6, 1, 2, 0, 0],\n",
        "    [4, 2, 7, 7, 9],\n",
        "    [3, 4, 0, 0, 0],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udC0SMjkKaCN"
      },
      "source": [
        "We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sACcGN4XYMgj"
      },
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each data example is a sentence, including its words and NER tags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading from datapath.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sentences: List[Sentence] = []\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "        PAD = \"<PAD>\"  # Special token used for padding\n",
        "\n",
        "        print(\"Loading data from %s\" % datapath)\n",
        "        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
        "        print(\"%d sentences loaded.\" % len(self.sentences))\n",
        "\n",
        "        if words_vocab is None:\n",
        "            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n",
        "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
        "\n",
        "        self.words_vocab = words_vocab\n",
        "\n",
        "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
        "        self.pad_idx = self.words_vocab[PAD]\n",
        "\n",
        "        if tags_vocab is None:\n",
        "            tags_vocab = vocab(tag_cnt, specials=[])\n",
        "        self.tags_vocab = tags_vocab\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Sentence:\n",
        "        \"\"\"\n",
        "        Get the idx'th sentence in the dataset.\n",
        "        \"\"\"\n",
        "        return self.sentences[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this method\n",
        "        # START HERE\n",
        "        return len(self.sentences)\n",
        "        # END\n",
        "\n",
        "    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        A customized function for batching a number of sentences together.\n",
        "        Different sentences have different lengths. Let max_len be the longest length.\n",
        "        When packing them into one tensor, we need to pad all sentences to max_len.\n",
        "        Return values:\n",
        "            `words`: a list in which each element itself is a list of words in a sentence\n",
        "            `word_idxs`: a batch_size x max_len tensor.\n",
        "                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n",
        "            `tags`: a list in which each element itself is a list of tags in a sentence\n",
        "            `tag_idxs`: a batch_size x max_len tensor\n",
        "                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n",
        "                        Otherwise, valid[i][j] is False.\n",
        "        \"\"\"\n",
        "        words: List[List[str]] = []\n",
        "        tags: List[List[str]] = []\n",
        "        max_len = -1  # length of the longest sentence\n",
        "        for sent in sentences:\n",
        "            words.append([])\n",
        "            tags.append([])\n",
        "            for w, t in sent:\n",
        "                words[-1].append(w)\n",
        "                tags[-1].append(t)\n",
        "            max_len = max(max_len, len(words[-1]))\n",
        "\n",
        "        batch_size = len(sentences)\n",
        "        word_idxs = torch.full(\n",
        "            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n",
        "        )\n",
        "        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n",
        "        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n",
        "\n",
        "        ## TODO: Fill in the values in word_idxs, tag_idxs, and valid_mask\n",
        "        ## Caveat: There may be out-of-vocabulary words in validation data\n",
        "        ## See torchtext.vocab.Vocab: https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab\n",
        "        ## START HERE\n",
        "        for i, (sentence_words, sentence_tags) in enumerate(zip(words, tags)):\n",
        "          for j, (word, tag) in enumerate(zip(sentence_words, sentence_tags)):\n",
        "              word_idx = self.words_vocab[word]\n",
        "              tag_idx = self.tags_vocab[tag]\n",
        "              word_idxs[i][j] = word_idx\n",
        "              tag_idxs[i][j] = tag_idx\n",
        "              valid_mask[i][j] = True\n",
        "\n",
        "        # END\n",
        "\n",
        "        return {\n",
        "            \"words\": words,\n",
        "            \"word_idxs\": word_idxs,\n",
        "            \"tags\": tags,\n",
        "            \"tag_idxs\": tag_idxs,\n",
        "            \"valid_mask\": valid_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "def create_sequence_dataloaders(\n",
        "    batch_size: int, shuffle: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Vocab]:\n",
        "    \"\"\"\n",
        "    Create the dataloaders for training and validaiton.\n",
        "    \"\"\"\n",
        "    ds_train = SequenceDataset(\"eng.train\")\n",
        "    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
        "    loader_train = DataLoader(\n",
        "        ds_train,\n",
        "        batch_size,\n",
        "        shuffle,\n",
        "        collate_fn=ds_train.form_batch,  # customized function for batching\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    loader_val = DataLoader(\n",
        "        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n",
        "    )\n",
        "    return loader_train, loader_val, ds_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2EcVxYuYvGv"
      },
      "source": [
        "Here is a simple sanity-check. Try to understand its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TazmodGWYx2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6ce88e-67a4-4ebd-ca01-dd841982a317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "Iterating on the training data..\n",
            "{'words': [['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '0000-00-00']], 'word_idxs': tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
            "        [11, 12,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [13, 14,  0,  0,  0,  0,  0,  0,  0]]), 'tags': [['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'], ['PER', 'PER'], ['LOC', 'O']], 'tag_idxs': tensor([[0, 1, 2, 1, 1, 1, 2, 1, 1],\n",
            "        [3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [4, 1, 1, 1, 1, 1, 1, 1, 1]]), 'valid_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False]])}\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def check_sequence_dataloader() -> None:\n",
        "    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n",
        "    print(\"Iterating on the training data..\")\n",
        "    for i, data_batch in enumerate(loader_train):\n",
        "        if i == 0:\n",
        "            print(data_batch)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "check_sequence_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifk3i-obY8YB"
      },
      "source": [
        "### Implement the Model **(8 points)**\n",
        "\n",
        "Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3V0NvQynZF8e"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Long short-term memory for NER\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool) -> None:\n",
        "        \"\"\"\n",
        "        Initialize an LSTM\n",
        "        Parameters:\n",
        "            `words_vocab`: vocabulary of words\n",
        "            `tags_vocab`: vocabulary of tags\n",
        "            `d_emb`: dimension of word embeddings (D)\n",
        "            `d_hidden`: dimension of the hidden layer (H)\n",
        "            `bidirectional`: true if LSTM should be bidirectional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Create the word embeddings (nn.Embedding),\n",
        "        #       the LSTM (nn.LSTM) and the output layer (nn.Linear).\n",
        "        #       Read the torch docs for additional guidance : https://pytorch.org/docs/stable\n",
        "        #       Note: Pay attention to the LSTM output shapes!\n",
        "        # START HERE\n",
        "\n",
        "         # Initialize the word embeddings layer\n",
        "        self.embedding = nn.Embedding(len(words_vocab), d_emb)\n",
        "\n",
        "        # Initialize the LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=d_emb, hidden_size=d_hidden, bidirectional=bidirectional)\n",
        "\n",
        "        # Initialize the output layer\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        self.hidden2tag = nn.Linear(d_hidden * num_directions, len(tags_vocab))\n",
        "\n",
        "        # END\n",
        "\n",
        "    def forward(\n",
        "        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given words in sentences, predict the logits of the NER tag.\n",
        "        Parameters:\n",
        "            `word_idxs`: a batch_size x max_len tensor\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "        Return values:\n",
        "            `logits`: a batch_size x max_len x 5 tensor\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        # START HERE\n",
        "        embeddings = self.embedding(word_idxs)\n",
        "\n",
        "        # Pack Padded Sequence\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(embeddings, valid_mask.sum(1).tolist(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, _ = self.lstm(packed_input)\n",
        "\n",
        "        #Unpack Padded Sequence\n",
        "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        # lstm_output, _ = self.lstm(embeddings)\n",
        "        # logits = self.output_layer(lstm_out)\n",
        "        logits = self.hidden2tag(lstm_out)\n",
        "        # END\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFTKaB4Zydx"
      },
      "source": [
        "We do a sanity-check by loading a batch of data examples and pass it through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PKg1ni4QZ6D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2264b3d5-72b6-4d20-b6f2-3ee8afbf171d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, bidirectional=True)\n",
            "  (hidden2tag): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Input word_idxs shape: torch.Size([4, 37])\n",
            "Input valid_mask shape: torch.Size([4, 37])\n",
            "Output logits shape: torch.Size([4, 37, 5])\n"
          ]
        }
      ],
      "source": [
        "def check_lstm() -> None:\n",
        "    # Hyperparameters\n",
        "    batch_size = 4\n",
        "    d_emb = 64\n",
        "    d_hidden = 128\n",
        "    bidirectional = True\n",
        "    # Create the dataloaders and the model\n",
        "    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n",
        "    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional)\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Get the first batch\n",
        "    data_batch = next(iter(loader_train))\n",
        "    # Move data to GPU\n",
        "    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "    # Calculate the model\n",
        "    print(\"Input word_idxs shape:\", word_idxs.size())\n",
        "    print(\"Input valid_mask shape:\", valid_mask.size())\n",
        "    logits = model(word_idxs, valid_mask)\n",
        "    print(\"Output logits shape:\", logits.size())\n",
        "\n",
        "\n",
        "check_lstm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddDYUiLY-hc"
      },
      "source": [
        "### Training and Validation **(6 points)**\n",
        "\n",
        "Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hv_15mnXZ_dy"
      },
      "outputs": [],
      "source": [
        "def train_lstm(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    silent: bool = False,  # whether to print the training loss\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train the LSTM model.\n",
        "    Return values:\n",
        "        1. the average training loss\n",
        "        2. training metrics such as accuracy and F1 score\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "    report_interval = 100\n",
        "\n",
        "    for i, data_batch in enumerate(loader):\n",
        "        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "        # TODO: Do the same tasks as train_ffnn\n",
        "        # START HERE\n",
        "        # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(word_idxs,valid_mask)\n",
        "        # Masked loss calculation\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), tag_idxs.flatten(), ignore_index=-1, reduction='mean')\n",
        "        # END\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # we get (unmasked) predictions by getting argmax of logits along last dimension\n",
        "        net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "        # flattening tensors for ease of processing\n",
        "        tag_idxs_flat = tag_idxs.flatten()\n",
        "        valid_mask_flat = valid_mask.flatten()\n",
        "        net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "        if not silent and i > 0 and i % report_interval == 0:\n",
        "            print(\n",
        "                \"\\t[%06d/%06d] Loss: %f\"\n",
        "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
        "            )\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def validate_lstm(\n",
        "    model: nn.Module, loader: DataLoader, device: torch.device\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    Return the validation loss and metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data_batch in loader:\n",
        "            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "            # TODO: Do the same tasks as validate_ffnn\n",
        "            # START HERE\n",
        "            # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "            # Forward pass\n",
        "            logits = model(word_idxs,valid_mask)\n",
        "\n",
        "            # Masked loss calculation\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), tag_idxs.flatten(), ignore_index=-1, reduction='mean')\n",
        "\n",
        "            # END\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # we get (unmasked) predictions by getting argmax of logits (You will need to define logits!)\n",
        "            net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "            # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "            tag_idxs_flat = tag_idxs.flatten()\n",
        "            valid_mask_flat = valid_mask.flatten()\n",
        "            net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Train and validate the LSTM model for a number of epochs.\n",
        "    \"\"\"\n",
        "    print(\"Hyperparameters:\", hyperparams)\n",
        "    # Create the dataloaders\n",
        "    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n",
        "        hyperparams[\"batch_size\"]\n",
        "    )\n",
        "    # Create the model\n",
        "    model = LSTM(\n",
        "        ds_train.words_vocab,\n",
        "        ds_train.tags_vocab,\n",
        "        hyperparams[\"d_emb\"],\n",
        "        hyperparams[\"d_hidden\"],\n",
        "        hyperparams[\"bidirectional\"],\n",
        "    )\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Create the optimizer\n",
        "    optimizer = optim.RMSprop(\n",
        "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
        "    )\n",
        "\n",
        "    # Train and validate\n",
        "    for i in range(hyperparams[\"num_epochs\"]):\n",
        "        print(\"Epoch #%d\" % i)\n",
        "\n",
        "        print(\"Training..\")\n",
        "        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n",
        "        print(\"Training loss: \", loss_train)\n",
        "        print(\"Training metrics:\")\n",
        "        for k, v in metrics_train.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "        print(\"Validating..\")\n",
        "        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n",
        "        print(\"Validation loss: \", loss_val)\n",
        "        print(\"Validation metrics:\")\n",
        "        for k, v in metrics_val.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9Nef7yal_M"
      },
      "source": [
        "Run the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pFxQxlokai6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "329d6f8f-7d42-4dd6-e0af-789f2aa76864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': True, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, bidirectional=True)\n",
            "  (hidden2tag): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  1.1302475134531658\n",
            "Training metrics:\n",
            "\t accuracy :  0.7991232439972104\n",
            "\t f1 :  [0.21744883 0.89126005 0.01874116 0.24251954 0.26126126]\n",
            "\t average f1 :  0.32624616806334716\n",
            "\t confusion matrix :  [[  1397   7409     46    644    354]\n",
            " [  1150 154401    957   6846   3892]\n",
            " [   114   3938     53    187    226]\n",
            " [   196   7800     49   2622    284]\n",
            " [   142   5684     33    373   1943]]\n",
            "Validating..\n",
            "Validation loss:  0.8559768966266087\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9040459601515707\n",
            "\t f1 :  [0.5171123  0.95557587 0.18584071 0.63876172 0.63334313]\n",
            "\t average f1 :  0.5861267464718287\n",
            "\t confusion matrix :  [[  967   988     2   170   123]\n",
            " [  168 40762    14   135    85]\n",
            " [   70   691   105    49    92]\n",
            " [  189   987     0  1465    49]\n",
            " [   96   722     2    78  1077]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.7345831217589202\n",
            "Training metrics:\n",
            "\t accuracy :  0.9253210322902381\n",
            "\t f1 :  [0.64545136 0.96824302 0.47328005 0.72518014 0.72892077]\n",
            "\t average f1 :  0.7082150692728832\n",
            "\t confusion matrix :  [[  5527   2997    166    604    568]\n",
            " [   418 165678     97    609    252]\n",
            " [   450   1728   1510    244    559]\n",
            " [   429   3045     31   7246    202]\n",
            " [   440   1722     86    328   5589]]\n",
            "Validating..\n",
            "Validation loss:  0.6474282996995109\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9318135517255429\n",
            "\t f1 :  [0.61334804 0.97465074 0.59767299 0.76773356 0.78463727]\n",
            "\t average f1 :  0.7476085217547763\n",
            "\t confusion matrix :  [[ 1668   428    34    54    66]\n",
            " [  601 40429    47    54    33]\n",
            " [  177   277   488    18    47]\n",
            " [  461   437     2  1775    15]\n",
            " [  282   226    55    33  1379]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.5604909106537148\n",
            "Training metrics:\n",
            "\t accuracy :  0.9616314485536397\n",
            "\t f1 :  [0.80425352 0.98636392 0.74106042 0.87448031 0.85508517]\n",
            "\t average f1 :  0.8522486675316723\n",
            "\t confusion matrix :  [[  7601   1230    244    391    411]\n",
            " [   349 166117    117    367    102]\n",
            " [   337    736   3005    148    304]\n",
            " [   319   1094     51   9360    118]\n",
            " [   419    598    163    199   6827]]\n",
            "Validating..\n",
            "Validation loss:  0.5174712836742401\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9460538646457238\n",
            "\t f1 :  [0.68484362 0.97999082 0.69327501 0.81630937 0.83494133]\n",
            "\t average f1 :  0.8018720317905188\n",
            "\t confusion matrix :  [[ 1708   350    41    48   103]\n",
            " [  407 40553    88    48    68]\n",
            " [  103   205   634    11    54]\n",
            " [  355   351     8  1942    34]\n",
            " [  165   139    51    19  1601]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.44634667921949317\n",
            "Training metrics:\n",
            "\t accuracy :  0.9780879176788057\n",
            "\t f1 :  [0.88247508 0.99292091 0.84675264 0.9357569  0.91066998]\n",
            "\t average f1 :  0.9137151035747546\n",
            "\t confusion matrix :  [[  8500    628    181    219    305]\n",
            " [   218 166490     86    185     57]\n",
            " [   222    413   3644     88    162]\n",
            " [   172    501     40  10116     84]\n",
            " [   319    286    127    100   7340]]\n",
            "Validating..\n",
            "Validation loss:  0.4269737047808511\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9515136698855071\n",
            "\t f1 :  [0.71084577 0.98200308 0.72641509 0.84872771 0.85773732]\n",
            "\t average f1 :  0.8251457957940176\n",
            "\t confusion matrix :  [[ 1786   276    38    57    93]\n",
            " [  436 40460   121    79    68]\n",
            " [   94   168   693    16    36]\n",
            " [  303   233    12  2118    24]\n",
            " [  156   102    37    31  1649]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.3635675487694917\n",
            "Training metrics:\n",
            "\t accuracy :  0.9866393573336523\n",
            "\t f1 :  [0.92134024 0.996056   0.89733318 0.96982443 0.94162421]\n",
            "\t average f1 :  0.9452356129895211\n",
            "\t confusion matrix :  [[  8978    382    135    130    258]\n",
            " [   157 166809     60     80     32]\n",
            " [   160    276   3920     41    123]\n",
            " [    87    197     25  10606     42]\n",
            " [   224    137     77     58   7670]]\n",
            "Validating..\n",
            "Validation loss:  0.36150682823998587\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9517377663692295\n",
            "\t f1 :  [0.71292413 0.98251405 0.72828283 0.84474886 0.86421081]\n",
            "\t average f1 :  0.8265361349112306\n",
            "\t confusion matrix :  [[ 1870   248    39    32    61]\n",
            " [  454 40484   149    37    40]\n",
            " [  104   155   721     9    18]\n",
            " [  375   251    11  2035    18]\n",
            " [  193   107    53    15  1607]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.30112115210956997\n",
            "Training metrics:\n",
            "\t accuracy :  0.9924118538608807\n",
            "\t f1 :  [0.95557589 0.99766838 0.93566591 0.98667765 0.96743212]\n",
            "\t average f1 :  0.9686039928887815\n",
            "\t confusion matrix :  [[  9400    245     66     60    127]\n",
            " [   106 166876     53     29     21]\n",
            " [    96    189   4145     23     76]\n",
            " [    50     67      9  10776     23]\n",
            " [   124     70     58     30   7857]]\n",
            "Validating..\n",
            "Validation loss:  0.30716635499681744\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9577272542069022\n",
            "\t f1 :  [0.7549505  0.9833303  0.75259875 0.87480127 0.87612671]\n",
            "\t average f1 :  0.8483615056285252\n",
            "\t confusion matrix :  [[ 1830   252    36    45    87]\n",
            " [  350 40555   121    67    71]\n",
            " [   81   162   724    11    29]\n",
            " [  214   246     9  2201    20]\n",
            " [  123   106    27    18  1701]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.2516991490567172\n",
            "Training metrics:\n",
            "\t accuracy :  0.9956824991649176\n",
            "\t f1 :  [0.97430669 0.99858529 0.96302935 0.99447716 0.98142036]\n",
            "\t average f1 :  0.9823637708540927\n",
            "\t confusion matrix :  [[  9556    161     44     25     70]\n",
            " [    70 166936     28     14     22]\n",
            " [    59    110   4298      9     45]\n",
            " [    13     24      5  10894     14]\n",
            " [    62     44     30     17   8029]]\n",
            "Validating..\n",
            "Validation loss:  0.2658194324799946\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9596422605223486\n",
            "\t f1 :  [0.78021263 0.98371644 0.74692875 0.88174905 0.87969349]\n",
            "\t average f1 :  0.8544600701237967\n",
            "\t confusion matrix :  [[ 1798   243    42    70    97]\n",
            " [  270 40506   174   139    75]\n",
            " [   64   141   760    17    25]\n",
            " [  132   206    12  2319    21]\n",
            " [   95    93    40    25  1722]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.21498120769306464\n",
            "Training metrics:\n",
            "\t accuracy :  0.9967264900200297\n",
            "\t f1 :  [0.98052412 0.99899573 0.97542533 0.99557179 0.98296777]\n",
            "\t average f1 :  0.9866969465836049\n",
            "\t confusion matrix :  [[  9616    109     27     10     68]\n",
            " [    46 167117     27      7     18]\n",
            " [    37     74   4386      4     28]\n",
            " [    15     19      4  10904     18]\n",
            " [    70     36     20     20   8022]]\n",
            "Validating..\n",
            "Validation loss:  0.2321928377662386\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9592959295929593\n",
            "\t f1 :  [0.78504264 0.9833384  0.75096899 0.87760322 0.88193018]\n",
            "\t average f1 :  0.8557766876548353\n",
            "\t confusion matrix :  [[ 1795   233    42    90    90]\n",
            " [  278 40398   188   225    75]\n",
            " [   55   127   775    29    21]\n",
            " [   98   161    12  2402    17]\n",
            " [   97    82    40    38  1718]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.18339734110567305\n",
            "Training metrics:\n",
            "\t accuracy :  0.9973553554053043\n",
            "\t f1 :  [0.98571573 0.99894243 0.97943215 0.99608949 0.99051814]\n",
            "\t average f1 :  0.9901395866077243\n",
            "\t confusion matrix :  [[  9730     93     18      9     33]\n",
            " [    57 166715     26     23     16]\n",
            " [    21     83   4381      5     18]\n",
            " [    14     25      1  10953      6]\n",
            " [    37     30     12      3   8096]]\n",
            "Validating..\n",
            "Validation loss:  0.2042675997529711\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9631463146314632\n",
            "\t f1 :  [0.79779736 0.98446999 0.7802645  0.89303972 0.8855877 ]\n",
            "\t average f1 :  0.8682318508473742\n",
            "\t confusion matrix :  [[ 1811   285    36    42    76]\n",
            " [  207 40729   117    59    52]\n",
            " [   48   167   767     8    17]\n",
            " [  114   279     8  2271    18]\n",
            " [  110   119    31    16  1699]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.1561865442328983\n",
            "Training metrics:\n",
            "\t accuracy :  0.9986972992667941\n",
            "\t f1 :  [0.99302114 0.99948141 0.98934044 0.99840728 0.99510883]\n",
            "\t average f1 :  0.9950718190474188\n",
            "\t confusion matrix :  [[  9818     46      7      6     18]\n",
            " [    28 166711     18      8      7]\n",
            " [     9     42   4455      2      8]\n",
            " [     6      6      3  10970      4]\n",
            " [    18     18      7      0   8138]]\n",
            "Validating..\n",
            "Validation loss:  0.18301835443292344\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9634722731532412\n",
            "\t f1 :  [0.80971098 0.98453689 0.7619517  0.895      0.88348271]\n",
            "\t average f1 :  0.8669364580202631\n",
            "\t confusion matrix :  [[ 1751   303    51    53    92]\n",
            " [  139 40717   141    94    73]\n",
            " [   30   165   773    13    26]\n",
            " [   80   248    21  2327    14]\n",
            " [   75   116    36    23  1725]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.13814072338519273\n",
            "Training metrics:\n",
            "\t accuracy :  0.9969568167305236\n",
            "\t f1 :  [0.99051988 0.99840876 0.99061707 0.98524394 0.99401272]\n",
            "\t average f1 :  0.9917604705671608\n",
            "\t confusion matrix :  [[  9717    106     12      7     17]\n",
            " [    21 166899     11     25     11]\n",
            " [     8     39   4487      1      4]\n",
            " [     4    276      3  10683      3]\n",
            " [    11     43      7      1   8052]]\n",
            "Validating..\n",
            "Validation loss:  0.21112979097025736\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8994010512162327\n",
            "\t f1 :  [0.73125822 0.95045114 0.71218487 0.62367427 0.7593117 ]\n",
            "\t average f1 :  0.7553760383562408\n",
            "\t confusion matrix :  [[ 1668   105    20   286   171]\n",
            " [  469 37501   181  2424   589]\n",
            " [   54    73   678   128    74]\n",
            " [   40    48     5  2558    39]\n",
            " [   81    21    13   117  1743]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.12174059919737003\n",
            "Training metrics:\n",
            "\t accuracy :  0.9947197947806818\n",
            "\t f1 :  [0.98020955 0.99763189 0.98036591 0.97696145 0.98466801]\n",
            "\t average f1 :  0.983967362059683\n",
            "\t confusion matrix :  [[  9683    106     20     28     55]\n",
            " [    95 166405     36    299     53]\n",
            " [    13     54   4394     11     27]\n",
            " [    30    112      3  10771      9]\n",
            " [    44     35     12     16   8060]]\n",
            "Validating..\n",
            "Validation loss:  0.14231552822249277\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9668337204090779\n",
            "\t f1 :  [0.82870901 0.98593525 0.79937952 0.89909977 0.89107884]\n",
            "\t average f1 :  0.8808404800956167\n",
            "\t confusion matrix :  [[ 1807   273    35    63    72]\n",
            " [  126 40763    87   129    59]\n",
            " [   33   160   773    27    14]\n",
            " [   61   208     6  2397    18]\n",
            " [   84   121    26    26  1718]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.10297473188903597\n",
            "Training metrics:\n",
            "\t accuracy :  0.9990416196627699\n",
            "\t f1 :  [0.99502942 0.99955346 0.99489796 0.99794886 0.99718344]\n",
            "\t average f1 :  0.9969226267377863\n",
            "\t confusion matrix :  [[  9809     43      4      6      4]\n",
            " [    23 166762     10     10      3]\n",
            " [     4     22   4485      0      1]\n",
            " [     3     19      2  10947      4]\n",
            " [    11     19      3      1   8143]]\n",
            "Validating..\n",
            "Validation loss:  0.13010359874793462\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9668540928166891\n",
            "\t f1 :  [0.82979727 0.98560847 0.80230729 0.89781431 0.89343215]\n",
            "\t average f1 :  0.8817918963042256\n",
            "\t confusion matrix :  [[ 1760   301    34    69    86]\n",
            " [   97 40783    80   136    68]\n",
            " [   28   168   765    30    16]\n",
            " [   43   221     3  2403    20]\n",
            " [   64   120    18    25  1748]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.08911273711257511\n",
            "Training metrics:\n",
            "\t accuracy :  0.999560786192715\n",
            "\t f1 :  [0.99767018 0.99980824 0.99676014 0.99917951 0.99883614]\n",
            "\t average f1 :  0.9984508415758399\n",
            "\t confusion matrix :  [[  9849     18      2      4      2]\n",
            " [     8 166847      7      3      2]\n",
            " [     5     12   4461      0      2]\n",
            " [     4      6      0  10960      1]\n",
            " [     3      8      1      0   8153]]\n",
            "Validating..\n",
            "Validation loss:  0.12031068972178868\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9662429205883551\n",
            "\t f1 :  [0.82807348 0.98511607 0.80336488 0.89457831 0.8922759 ]\n",
            "\t average f1 :  0.8806817273335993\n",
            "\t confusion matrix :  [[ 1758   321    34    59    78]\n",
            " [   93 40804    74   132    61]\n",
            " [   24   174   764    29    16]\n",
            " [   45   254     1  2376    14]\n",
            " [   76   124    22    26  1727]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.07785816821787092\n",
            "Training metrics:\n",
            "\t accuracy :  0.9997203118522847\n",
            "\t f1 :  [0.9984804  0.99988302 0.99822774 0.99936038 0.99920679]\n",
            "\t average f1 :  0.9990316642511408\n",
            "\t confusion matrix :  [[  9856     13      0      3      1]\n",
            " [     4 166680      2      0      3]\n",
            " [     3      9   4506      1      0]\n",
            " [     5      2      1  10937      2]\n",
            " [     1      6      0      0   8188]]\n",
            "Validating..\n",
            "Validation loss:  0.11124209101711001\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9660391965122438\n",
            "\t f1 :  [0.82925682 0.98506417 0.79916099 0.89264899 0.89233153]\n",
            "\t average f1 :  0.8796925001654605\n",
            "\t confusion matrix :  [[ 1763   312    33    64    78]\n",
            " [   93 40792    80   136    63]\n",
            " [   28   174   762    28    15]\n",
            " [   45   256     1  2374    14]\n",
            " [   73   123    24    27  1728]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": True,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vA-Yjqg7n0V"
      },
      "source": [
        "We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7wNrdvJ98ARB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352ddf5e-b59a-4828-bc3f-d990544c6c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': False, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128)\n",
            "  (hidden2tag): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  1.0666369442586545\n",
            "Training metrics:\n",
            "\t accuracy :  0.8188000777810464\n",
            "\t f1 :  [0.10591308 0.90509007 0.05016657 0.23357769 0.31682073]\n",
            "\t average f1 :  0.32231362966041377\n",
            "\t confusion matrix :  [[   892   8008     81    399    481]\n",
            " [  5182 159571    288    557   1409]\n",
            " [   206   3752    128    144    293]\n",
            " [   413   8620     45   1625    266]\n",
            " [   290   5650     38    220   2005]]\n",
            "Validating..\n",
            "Validation loss:  0.8415506907871791\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8954488041396732\n",
            "\t f1 :  [0.30327592 0.9558882  0.15957447 0.55172414 0.64955417]\n",
            "\t average f1 :  0.5240033802142519\n",
            "\t confusion matrix :  [[  449  1159    13   365   264]\n",
            " [   57 40869     6   184    48]\n",
            " [   77   583    90   106   151]\n",
            " [   54  1230     1  1344    61]\n",
            " [   74   505    11   183  1202]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.7222525874773661\n",
            "Training metrics:\n",
            "\t accuracy :  0.9223789117002192\n",
            "\t f1 :  [0.55885036 0.97252412 0.53547038 0.69112045 0.71985252]\n",
            "\t average f1 :  0.6955635659498836\n",
            "\t confusion matrix :  [[  4900   2803    305    985    906]\n",
            " [   521 165545    127    708    235]\n",
            " [   621   1206   1921    334    449]\n",
            " [   875   2562    123   7075    313]\n",
            " [   720   1192    168    424   5662]]\n",
            "Validating..\n",
            "Validation loss:  0.635746785572597\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9310801450515422\n",
            "\t f1 :  [0.58362989 0.97197239 0.62141603 0.73970868 0.75475862]\n",
            "\t average f1 :  0.7342971222602819\n",
            "\t confusion matrix :  [[ 1148   731    56   136   179]\n",
            " [  123 40904    55    59    23]\n",
            " [   81   314   531    35    46]\n",
            " [  147   734    23  1752    34]\n",
            " [  185   320    37    65  1368]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.5498052272531722\n",
            "Training metrics:\n",
            "\t accuracy :  0.9561799098056432\n",
            "\t f1 :  [0.74524833 0.98730077 0.728236   0.84753845 0.82074704]\n",
            "\t average f1 :  0.8258141209715193\n",
            "\t confusion matrix :  [[  7195   1157    342    539    676]\n",
            " [   437 165830    138    401    132]\n",
            " [   434    541   3024    214    287]\n",
            " [   634    959    123   9064    168]\n",
            " [   700    501    178    223   6559]]\n",
            "Validating..\n",
            "Validation loss:  0.5111438589436668\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9410422523733855\n",
            "\t f1 :  [0.66079295 0.97573603 0.67125645 0.78485726 0.79489269]\n",
            "\t average f1 :  0.7775070780243871\n",
            "\t confusion matrix :  [[ 1350   575    51   112   162]\n",
            " [  123 40897    64    61    19]\n",
            " [   82   268   585    34    38]\n",
            " [   98   659    12  1897    24]\n",
            " [  183   265    24    40  1463]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.43985528305724814\n",
            "Training metrics:\n",
            "\t accuracy :  0.973300147639759\n",
            "\t f1 :  [0.83708095 0.99329813 0.81011184 0.92413477 0.87704766]\n",
            "\t average f1 :  0.8883346691947711\n",
            "\t confusion matrix :  [[  8190    607    260    295    509]\n",
            " [   293 166294    113    167     66]\n",
            " [   337    364   3477    137    230]\n",
            " [   334    394     63  10080     96]\n",
            " [   553    240    126    169   7094]]\n",
            "Validating..\n",
            "Validation loss:  0.42596172434943064\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9446481685205558\n",
            "\t f1 :  [0.70384803 0.97631642 0.71645022 0.78193548 0.8092613 ]\n",
            "\t average f1 :  0.7975622890157178\n",
            "\t confusion matrix :  [[ 1445   545    70    66   124]\n",
            " [   89 40976    47    37    15]\n",
            " [   55   250   662    12    28]\n",
            " [   83   750    21  1818    18]\n",
            " [  184   255    41    27  1468]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.35864432321654427\n",
            "Training metrics:\n",
            "\t accuracy :  0.9817036622644902\n",
            "\t f1 :  [0.88049801 0.99595047 0.86383903 0.95973001 0.90424093]\n",
            "\t average f1 :  0.920851691826674\n",
            "\t confusion matrix :  [[  8628    377    231    166    442]\n",
            " [   219 166257     70     85     48]\n",
            " [   252    233   3778     71    193]\n",
            " [   188    181     32  10522     62]\n",
            " [   467    139    109     98   7356]]\n",
            "Validating..\n",
            "Validation loss:  0.36760517954826355\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9455649268630567\n",
            "\t f1 :  [0.71669518 0.97574669 0.73462604 0.776403   0.82196249]\n",
            "\t average f1 :  0.8050866816595843\n",
            "\t confusion matrix :  [[ 1466   564    48    39   133]\n",
            " [   77 41016    42    17    12]\n",
            " [   57   254   663     6    27]\n",
            " [   78   821    14  1757    20]\n",
            " [  163   252    31    17  1512]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.2975060244401296\n",
            "Training metrics:\n",
            "\t accuracy :  0.9870896293007547\n",
            "\t f1 :  [0.91149179 0.99734005 0.89855726 0.97868253 0.92754509]\n",
            "\t average f1 :  0.9427233440592653\n",
            "\t confusion matrix :  [[  8939    258    180     96    380]\n",
            " [   168 166664     60     39     35]\n",
            " [   208    158   3986     38    148]\n",
            " [    94     89     15  10697     37]\n",
            " [   352     82     93     58   7585]]\n",
            "Validating..\n",
            "Validation loss:  0.3253999650478363\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9432424723953877\n",
            "\t f1 :  [0.70761326 0.97560686 0.73824018 0.77281724 0.77680141]\n",
            "\t average f1 :  0.79421578934796\n",
            "\t confusion matrix :  [[ 1515   555    44    50    86]\n",
            " [   76 41035    31    17     5]\n",
            " [   70   247   667     6    17]\n",
            " [   57   861    10  1757     5]\n",
            " [  314   260    48    27  1326]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.2494278892322823\n",
            "Training metrics:\n",
            "\t accuracy :  0.9901846797171671\n",
            "\t f1 :  [0.93077353 0.99805674 0.91956351 0.98750456 0.94283439]\n",
            "\t average f1 :  0.9557465440247354\n",
            "\t confusion matrix :  [[  9163    199    155     54    291]\n",
            " [   147 166663     45     22     19]\n",
            " [   161    115   4087     22    133]\n",
            " [    62     43      9  10827     26]\n",
            " [   294     59     75     36   7694]]\n",
            "Validating..\n",
            "Validation loss:  0.2911164334842137\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9453815751945565\n",
            "\t f1 :  [0.72031992 0.97526124 0.74684239 0.76983953 0.82237914]\n",
            "\t average f1 :  0.8069284436614035\n",
            "\t confusion matrix :  [[ 1441   568    52    53   136]\n",
            " [   66 41019    41    20    18]\n",
            " [   48   241   680    11    27]\n",
            " [   39   875    13  1751    12]\n",
            " [  157   252    28    24  1514]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.21359927786721122\n",
            "Training metrics:\n",
            "\t accuracy :  0.9899187316148975\n",
            "\t f1 :  [0.93566298 0.99726891 0.93003375 0.98085272 0.94968668]\n",
            "\t average f1 :  0.9587010077934881\n",
            "\t confusion matrix :  [[  9184    225    126     47    285]\n",
            " [   174 166693     41     91     35]\n",
            " [   139    114   4134     18    111]\n",
            " [    40    177      3  10732     17]\n",
            " [   227     56     70     26   7805]]\n",
            "Validating..\n",
            "Validation loss:  0.25790440823350635\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9459723750152793\n",
            "\t f1 :  [0.72727273 0.97498485 0.75323276 0.77076855 0.83146681]\n",
            "\t average f1 :  0.8115451387713737\n",
            "\t confusion matrix :  [[ 1456   577    63    40   114]\n",
            " [   72 41022    40    20    10]\n",
            " [   39   240   699     4    25]\n",
            " [   34   893    13  1735    15]\n",
            " [  153   253    34    13  1522]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.1812564796871609\n",
            "Training metrics:\n",
            "\t accuracy :  0.9932191380970676\n",
            "\t f1 :  [0.95165627 0.99850871 0.94928915 0.99324694 0.95922116]\n",
            "\t average f1 :  0.9703844475884397\n",
            "\t confusion matrix :  [[  9380    160     92     32    203]\n",
            " [   114 166721     42     15     23]\n",
            " [    89     76   4240     12     88]\n",
            " [    27     29      5  10884     14]\n",
            " [   236     39     49     14   7833]]\n",
            "Validating..\n",
            "Validation loss:  0.24157247373035975\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9439555066617773\n",
            "\t f1 :  [0.70503212 0.97446075 0.74156171 0.75848124 0.82801556]\n",
            "\t average f1 :  0.8015102778813279\n",
            "\t confusion matrix :  [[ 1317   586   104    43   200]\n",
            " [   56 40998    63    17    30]\n",
            " [   17   223   736     2    29]\n",
            " [   24   928    25  1688    25]\n",
            " [   72   246    50    11  1596]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.15652321886133264\n",
            "Training metrics:\n",
            "\t accuracy :  0.994412989798718\n",
            "\t f1 :  [0.95840758 0.99884473 0.95811576 0.99592733 0.96499295]\n",
            "\t average f1 :  0.9752576711336882\n",
            "\t confusion matrix :  [[  9413    126     97     19    203]\n",
            " [   105 166868     29     12     17]\n",
            " [    67     52   4312      5     71]\n",
            " [    19     16      2  10882      8]\n",
            " [   181     29     54      8   7870]]\n",
            "Validating..\n",
            "Validation loss:  0.2235724138362067\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9438332722161105\n",
            "\t f1 :  [0.71587813 0.97423593 0.75124654 0.75392201 0.82314588]\n",
            "\t average f1 :  0.8036856968438503\n",
            "\t confusion matrix :  [[ 1445   588    55    45   117]\n",
            " [   79 41009    32    23    21]\n",
            " [   51   239   678     6    33]\n",
            " [   40   938    10  1682    20]\n",
            " [  172   249    23    16  1515]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.1346333995461464\n",
            "Training metrics:\n",
            "\t accuracy :  0.99482170295133\n",
            "\t f1 :  [0.96119616 0.99903885 0.96124031 0.9962116  0.96576223]\n",
            "\t average f1 :  0.9766898304759541\n",
            "\t confusion matrix :  [[  9450    114     90     18    183]\n",
            " [    81 166827     22     11     20]\n",
            " [    65     41   4340      4     79]\n",
            " [    22      8      3  10913     13]\n",
            " [   190     24     46      4   7884]]\n",
            "Validating..\n",
            "Validation loss:  0.21044773714882986\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9421423623843865\n",
            "\t f1 :  [0.71725888 0.97315245 0.75173518 0.72702206 0.82359337]\n",
            "\t average f1 :  0.7985523892721433\n",
            "\t confusion matrix :  [[ 1413   594    67    42   134]\n",
            " [   62 41032    35    19    16]\n",
            " [   37   238   704     5    23]\n",
            " [   31  1047    14  1582    16]\n",
            " [  147   253    46    14  1515]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.11807473886896062\n",
            "Training metrics:\n",
            "\t accuracy :  0.9951569024105871\n",
            "\t f1 :  [0.96366124 0.99903705 0.96726224 0.99670962 0.96715908]\n",
            "\t average f1 :  0.9787658465404373\n",
            "\t confusion matrix :  [[  9507    105     61     15    186]\n",
            " [    94 167034     24      5     18]\n",
            " [    55     41   4358      5     57]\n",
            " [    19      8      2  10905      9]\n",
            " [   182     27     50      9   7922]]\n",
            "Validating..\n",
            "Validation loss:  0.21285050468785421\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9366621847369923\n",
            "\t f1 :  [0.63266476 0.97009778 0.74435696 0.70957471 0.82721382]\n",
            "\t average f1 :  0.7767816058495001\n",
            "\t confusion matrix :  [[ 1104   804    91   100   151]\n",
            " [   25 41072    33    17    17]\n",
            " [   14   257   709     8    19]\n",
            " [   14  1097     9  1560    10]\n",
            " [   83   282    56    22  1532]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.10277188707281042\n",
            "Training metrics:\n",
            "\t accuracy :  0.9950776711605337\n",
            "\t f1 :  [0.96331494 0.99872068 0.96787104 0.99630154 0.97197514]\n",
            "\t average f1 :  0.9796366691882439\n",
            "\t confusion matrix :  [[  9427    169     71     15    158]\n",
            " [    93 167063     36     13     25]\n",
            " [    41     54   4353      4     53]\n",
            " [    18     11      3  10910     10]\n",
            " [   153     27     27      7   7977]]\n",
            "Validating..\n",
            "Validation loss:  0.18762803077697754\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9418164038626085\n",
            "\t f1 :  [0.70429966 0.97299925 0.75519126 0.72685504 0.82492889]\n",
            "\t average f1 :  0.7968548189594518\n",
            "\t confusion matrix :  [[ 1335   611    62    40   202]\n",
            " [   51 41027    38    23    25]\n",
            " [   28   240   691     7    41]\n",
            " [   24  1041    14  1582    29]\n",
            " [  103   248    18    11  1595]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.08938392196540479\n",
            "Training metrics:\n",
            "\t accuracy :  0.9963393927287255\n",
            "\t f1 :  [0.97214471 0.99932231 0.97659976 0.99685032 0.97488236]\n",
            "\t average f1 :  0.9839598892298997\n",
            "\t confusion matrix :  [[  9580     73     48     17    149]\n",
            " [    64 166629     17      5     15]\n",
            " [    38     29   4403      6     42]\n",
            " [    16      7      2  10919     11]\n",
            " [   144     16     29      5   7976]]\n",
            "Validating..\n",
            "Validation loss:  0.1815204152039119\n",
            "Validation metrics:\n",
            "\t accuracy :  0.94112374200383\n",
            "\t f1 :  [0.71389237 0.97254363 0.75399361 0.71861876 0.82163343]\n",
            "\t average f1 :  0.7961363589250261\n",
            "\t confusion matrix :  [[ 1426   601    66    24   133]\n",
            " [   78 41018    38    17    13]\n",
            " [   39   233   708     5    22]\n",
            " [   41  1083    12  1540    14]\n",
            " [  161   253    47    10  1504]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.07846767096607773\n",
            "Training metrics:\n",
            "\t accuracy :  0.9965109729899466\n",
            "\t f1 :  [0.97318085 0.99941052 0.9748122  0.99671263 0.97718794]\n",
            "\t average f1 :  0.9842608267279698\n",
            "\t confusion matrix :  [[  9616     72     54     14    130]\n",
            " [    55 166997     13      4     13]\n",
            " [    49     23   4412      8     37]\n",
            " [    19      4     12  10915      5]\n",
            " [   137     13     32      6   7989]]\n",
            "Validating..\n",
            "Validation loss:  0.170707711151668\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9418164038626085\n",
            "\t f1 :  [0.71354972 0.97286404 0.75319149 0.72520473 0.82545753]\n",
            "\t average f1 :  0.7980534997005513\n",
            "\t confusion matrix :  [[ 1385   609    69    59   128]\n",
            " [   57 41032    36    25    14]\n",
            " [   30   244   708     5    20]\n",
            " [   24  1045    14  1594    13]\n",
            " [  136   259    46    23  1511]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "## TODO: Re-run with unidirectional LSTMs\n",
        "## Keep other hyperparameters fixed\n",
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": False,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})\n",
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8UChDyKaPBs"
      },
      "source": [
        "### Questions **(2 points)**\n",
        "\n",
        "(a) How does the final performance of LSTMs compare to FFNNs? Is it better? What is a possible explanation?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        "(b) How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)Comparing the final performance of LSTMs to FFNNs, we can observe that LSTMs generally achieve better performance because\n",
        "\n",
        "1) LSTMs are specifically designed to model sequential data by capturing long-term dependencies. In NER tasks, the context of each word is crucial for determining its entity label. LSTMs can effectively capture these dependencies across the entire sequence of words, whereas FFNNs treat each word independently, neglecting the sequential nature of the data.\n",
        "\n",
        "2)LSTMs can be easily modified to incorporate bidirectional processing, where information from both past and future contexts is considered. This bidirectional modeling allows LSTMs to make more informed predictions by leveraging information from both directions in the input sequence. FFNNs, in their standard form, process data in a unidirectional manner and might overlook contextual information from the future."
      ],
      "metadata": {
        "id": "yXE8zNkanHsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b)Unidirectional LSTM: Processes input sequences in a single direction, either forward or backward. This means it can only utilize past information (for forward LSTM) or future information (for backward LSTM) when making predictions at each time step.\n",
        "Bidirectional LSTM: Processes input sequences in both forward and backward directions simultaneously. It consists of two separate LSTM layers, one processing the input sequence forward and the other processing it backward. As a result, it can utilize both past and future information when making predictions, effectively capturing dependencies from both directions."
      ],
      "metadata": {
        "id": "YD7nnJldnoat"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kMEWxkN_bpIT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}